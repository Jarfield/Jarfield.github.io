{"posts":[{"title":"FastViT: A Fast Hybrid Vision Transformer  using Structural Reparameterization 阅读笔记","text":"金培晟 Jarfield 0 摘要近年来 Transformer 和卷积架构的结合促使视觉模型在准确率和效率上稳步提升。在这项工作中，作者提出了 FastViT，这是一种混合型视觉Transformer架构，在时延-准确率折中（latency-accuracy trade-off）上达到当前最佳水平。为此，作者设计了一种新颖的令牌混合（token mixing）算子 RepMixer 作为 FastViT 的基本模块。RepMixer 通过结构重参数化（structural reparameterization）移除网络中的跳跃连接（skip-connections），显著降低内存访问开销。此外，作者在训练期间引入线性过参数化（train-time overparameterization）和大卷积核（large kernel）卷积来提高模型准确率，并从实验证明这些改动对推理时延几乎没有负面影响。实验结果表明：在 ImageNet 数据集上，以相同准确率比较，FastViT 模型在移动设备上推理速度比最新的混合Transformer架构 CMT 快 3.5 倍，比 EfficientNet 快 4.9 倍，比 ConvNeXt 快 1.9 倍；在相近推理时延下，FastViT 的ImageNet Top-1准确率比 MobileOne 高出 4.2%。在图像分类、目标检测、语义分割和3D网格回归等多个任务上，FastViT 相比其他架构均取得更高的准确率且大幅降低推理时延（包括移动设备和桌面GPU）。同时，FastViT 对分布外样本和图像扰动具有更高的鲁棒性（robustness），优于现有的抗扰动模型。代码和预训练模型已开放至：FastViT。 1 Introduction近年来 Vision Transformer (ViT) 模型在图像分类、目标检测、语义分割等任务上取得了卓越表现，但其计算开销一直较高。为降低ViT的计算和内存需求，不少研究提出了高效Transformer改进。尤其是混合架构（hybrid architecture）的视觉Transformer成功融合了卷积神经网络（CNN）的局部建模优势和Transformer的全局建模能力，能够在众多视觉任务上取得极具竞争力的效果。FastViT 的目标是在保证精度的前提下，将推理时延（latency）显著降低，取得最佳的时延-准确率折中。 值得注意的是，许多近期视觉Transformer和混合架构都遵循 MetaFormer框架，即模块由“令牌混合+跳跃连接”以及“前馈网络（FFN）+跳跃连接”组成。然而，这些跳跃连接会显著增加推理延迟，因为它们带来了额外的内存访问成本。为解决这一瓶颈，作者引入RepMixer 模块——一种完全可重参数化的令牌混合算子，可在推理时重构网络结构以移除跳跃连接。RepMixer 模块内部采用深度卷积（depthwise convolution）对空间信息进行混合，灵感来自 ConvMixer；但与ConvMixer不同的是，RepMixer 不含非线性激活且在推理时能够融合分支，使其等效为单一卷积层，从而减少内存访问开销。 此外，为进一步提升效率（降低参数量和 FLOPs），FastViT 将网络中的所有 密集卷积替换为分解形式（depthwise卷积 + 逐点卷积），这一策略在MobileNet等高效架构中很常见。然而，简单地因式分解卷积会降低模型容量，影响准确率（见*(table1)*第一行与第三行比较）。为此，作者借鉴 MobileOne等工作的线性训练时过参数化方法，在训练阶段为部分卷积分支添加额外参数分支以提升容量（推理时再融合移除）。这些额外分支仅在训练时存在，在推理时通过重参数化融入主干，不增加推理开销。 最后，作者在网络中引入大卷积核卷积。原因在于，尽管自注意力（self-attention）可以有效建模长程依赖提升精度，但其在硬件上计算延迟高。相比之下，大卷积核能够扩大感受野且计算高效。因此，FastViT 在 FFN 层和补丁嵌入层中引入了大卷积核的深度卷积，提高性能的同时对总体时延影响很小。 综上，FastViT 基于三大设计原则构建：（i）使用 RepMixer 模块移除跳跃连接；（ii）在训练时对卷积层进行线性过参数化以提升容量；（iii）在模型中引入大卷积核卷积以扩大感受野但避免高昂的注意力计算。凭借这些设计，FastViT 在移动设备和桌面GPU上均实现了高效的通用视觉Transformer。Figure1 展示了不同模型在ImageNet精度和推理时延上的折线对比，FastViT 在移动端与GPU端均取得最佳的准确率-时延曲线，显著领先于此前的高效CNN和Transformer模型。 贡献总结： 提出FastViT架构：设计混合Transformer并通过结构重参数化移除跳跃连接，达到当前最优的精度-时延折中。 极快的推理速度：在两种常用平台上（移动设备和桌面GPU），FastViT 都实现了同精度下的最低延迟。 多任务泛化能力：FastViT 在图像分类、目标检测、语义分割和3D手部网格回归等多个任务上均取得领先性能，证明其通用性。 鲁棒性强：FastViT 对图像腐败和分布外数据具有高鲁棒性，在显著提速的同时超越了现有强调鲁棒性的模型。 2 Related Work过去十年中，卷积神经网络（CNN）一直是视觉模型的标准架构，如 ResNet 系列在分类和检测中表现优异。然而近年Transformer 技术兴起，使模型具备建模长程依赖的能力，在视觉任务中也取得巨大成功。Transformer 的全局自注意力提供了CNN不具备的全局信息，但代价是计算复杂度高、速度较慢。为此，众多工作尝试在视觉Transformer中降低自注意力的计算代价，如限制注意力范围或引入稀疏/低秩注意力等。 混合视觉Transformer： 为兼顾效率与精度，近期一些架构采用混合设计，将卷积与Transformer结合来同时捕获局部和全局特征。一些模型使用卷积改进ViT的补丁提取阶段或在网络前几层引入卷积提取低级特征；也有模型采用窗口化注意力在局部范围内计算自注意力以降低复杂度。此外，一系列显式混合架构被提出，通过交替或并行地使用卷积模块和Transformer模块来交换信息。值得注意的是，MetaFormer 架构表明，即使用简单的池化算子替代自注意力（如 PoolFormer 模型），也能获得不错的性能，提示令牌混合方式的多样化潜力。然而，大多数混合Transformer的令牌混合仍以自注意力为主，低效的全局操作限制了推理速度。 结构重参数化： 近年来，一些研究表明可以通过重参数化的技巧将训练时的复杂结构简化为推理时的高效结构，从而降低实际部署时的开销。例如 RepVGG和 MobileOne展示了将训练时的多分支卷积结构在推理时融合为单一卷积层的可行性，特别是移除跳跃连接能够减少推理时的内存访问成本。在 FastViT 中，作者借鉴这一思想，提出了可重参数化的RepMixer模块，在推理时去除MetaFormer中的跳跃连接，从而降低延迟。 此外，为提高效率，很多高效模型使用了卷积因式分解（如 depthwise + pointwise 卷积）来降低FLOPs。但这往往减少了参数量，带来模型容量下降的问题。线性过参数化技术近期被用于补偿这一缺陷：在训练阶段为卷积层添加线性分支增加参数容量，推理时再折叠融合。FastViT 在因式分解卷积的基础上应用了该方法，提高模型容量的同时保持高效的推理结构。 综上所述，据作者所知，此前尚无混合Transformer架构同时采用跳跃连接重参数化和线性过参数化来优化延迟与容量的工作。FastViT 将这些思想相结合，在保持或提升准确率的同时，大幅降低了不同硬件环境下的推理延迟。 3 Architecture3.1 OverviewFastViT 是一种卷积-Transformer混合架构，由四个分辨率逐渐降低的阶段组成，如Figure2所示；各 FastViT 模型的详细配置参见table2。整个网络遵循 MetaFormer 框架：每个阶段由一个令牌混合模块（Token Mixer）和一个前馈网络（FFN）堆叠而成，但与标准MetaFormer不同的是，FastViT 对这些模块的结构做了精心设计以优化速度。 如Figure2所示： RepMixer 模块： FastViT 的令牌混合模块采用 RepMixer，通过结构重参数化移除了常规MetaFormer中令牌混合后的跳跃连接，从而降低内存访问成本。此外，RepMixer利用深度卷积在空间维度混合信息（类似 ConvMixer），既保证局部建模能力又便于在推理时融合分支。 分解卷积 + 过参数化： 为提升效率和性能，FastViT 将网络中的 卷积（如Stem层和Patch Embedding层中的卷积）替换为因式分解卷积（depthwise + 1×1卷积）。单纯分解虽可减少参数和FLOPs，但可能降低模型表示能力。因此，作者对这些卷积层应用线性训练时过参数化（MobileOne风格）：在训练阶段添加平行的卷积分支增大容量，而在推理时将其折叠合并。 大卷积核卷积： FastViT 用大核深度卷积替代了部分自注意力，以扩大感受野且避免高计算延迟。具体地，在每个FFN层和Patch Embedding层内加入大卷积核的深度卷积（例如卷积），提升局部模块的感受野。由于这些位置处于网络早期或非主干，全局来看对时延影响不大但能带来准确率提升。 Table1展示了作者从 PoolFormer-S12 基础模型逐步引入上述改动得到 FastViT-S12 的性能变化：将输入分辨率由224增大到256略微提升准确率至77.6%；用RepMixer取代池化令牌混合几乎不增加FLOPs却显著降低延迟，并将Top-1准确率提高到78.5%；将密集卷积替换为因式分解卷积可减少约27%参数和6% FLOPs，但Top-1略有下降至78.0%；训练时对这些卷积添加过参数化分支在不改变推理成本的前提下将Top-1提升回78.9%；最后，在FFN和Patch Embedding中加入大核卷积带来额外+0.9%的准确率增益，使FastViT-S12达到79.8%的ImageNet Top-1准确率，同时移动端延迟仅约1.4ms。 值得注意的是，标准的自注意力令牌混合在高分辨率输入下计算量巨大、速度缓慢。虽然一些工作提出了更高效的注意力变体以减轻此问题，FastViT 选择用大卷积核卷积作为更高效的替代，以提升网络早期层次的感受野而几乎不增加延迟。Table1 中对比了是否使用大核卷积对精度和速度的影响：对FastViT-S12而言，在FFN和Patch Embedding中加入大核深度卷积使Top-1从78.9%提升到79.8%，移动端时延从1.26ms小幅增至1.40ms，可见以极小的延迟代价换来了0.9%的准确率提升。下一节将详细介绍FastViT各组件的设计细节。 3.2 FastViT3.2.1 Reparameterizing Skip ConnectionsRepMixer 模块 视觉Transformer中的令牌混合通常通过自注意力实现，也有工作探索纯卷积的混合方式。例如 ConvMixer采用深度卷积进行令牌混合，并使用跳跃连接融合输入，如： 其中 表示非线性激活函数，BN表示批归一化（Batch Normalization），DWConv表示深度卷积。ConvMixer 结构虽然有效，但其模式仍需要在推理时保留加法分支。为此，FastViT 提出将操作顺序重排并移除非线性激活： 这样设计的主要好处在于，该结构可在推理时重参数化为单一的深度卷积层，将加法分支融合进去，如下式所示： Figure2d 展示了这种重参数化过程：训练时RepMixer含有批归一化和残差分支，而推理时这些操作可并入卷积核权重，使整个RepMixer模块“折叠”成一个等效的深度卷积层。这种移除跳跃连接的设计大幅降低了推理时的内存访问开销。 位置编码 FastViT 采用条件位置编码（conditional positional encoding）来替代传统固定或可学习位置编码。具体实现上，通过一个Depthwise卷积根据每个令牌的局部邻域动态生成位置编码，并将其加到Patch Embedding的输出上。由于此过程不含任何非线性激活，整个位置编码模块也可以与相邻层一并重参数化融入模型（类似Figure2a 中的融合示意)。总之，RepMixer 模块配合条件位置编码，在保持模型表达能力的同时，实现了无跳跃连接的高效令牌混合。 实验结果 作者通过实验验证移除跳跃连接的收益。在一个MetaFormer S12架构中分别使用池化和RepMixer作为令牌混合模块（两者FLOPs均约1.8G），测试不同输入分辨率下的推理延迟。Figure3展示了两者在 iPhone 12 Pro 上的延迟对比曲线：随着分辨率升高，RepMixer的优势愈发明显。在输入时，使用RepMixer比池化将延迟降低约25.1%，而在超高分辨率如时延迟降低达43.9%。这证明移除跳跃连接对于高分辨率输入的效率提升至关重要，而RepMixer模块成功在不损失精度的情况下实现了这一点。 3.2.2 Linear Train-time Overparameterization将卷积分解为 Depthwise 和 Pointwise 虽有效降低了参数和FLOPs，但也削弱了模型容量和准确率。为此，FastViT 在训练阶段对这些因式分解卷积引入线性过参数化分支，以提高其拟合能力。具体做法是：对于每个用因式分解卷积分解的层（如Stem层、Patch Embedding层以及投影层），在训练时额外添加若干并行的卷积分支（通常使用卷积实现线性增加，不改变非线性特征），并将其输出累加到主干分支上。这些附加卷积分支的参数在训练中学习，以弥补因式分解带来的容量下降。在推理阶段，这些并行卷积可以与主卷积核权重相加合并，完全移除额外分支，不增加任何推理成本。 Table3 对比了开启和关闭训练时过参数化对FastViT模型精度和训练时间的影响。例如，对于FastViT-SA12，在ImageNet-1k上不使用过参数化训练的Top-1准确率为80.0%，总训练时长31.3小时；加入过参数化分支训练后，准确率提高到80.6%，训练耗时增加到33.4小时（约+6.7%）。类似地，较大的FastViT-SA36准确率从83.3%提升至83.6%，训练耗时增加约4.4%。由此可见，训练时过参数化可以以很小的额外训练代价换取**+0.5%~0.6%**的准确率提升。值得注意的是，FastViT 仅对那些由密集卷积替换为因式分解卷积的层应用过参数化（如前文提到的Stem、Patch Embedding和Projection层）。这些层在整个网络中计算成本占比较低，因此对它们进行过参数化不会显著拖慢训练。例如，在相同设置下，应用过参数化的FastViT-S12训练时间比原模型仅增加约6.7%，FastViT-SA36增加约4.4%。因此，线性过参数化策略有效提升了模型容量和准确率，同时对推理效率无任何影响，对训练开销的影响也在可接受范围内。 3.2.3 Large Kernel Convolutions由于 RepMixer 等卷积型令牌混合提供的感受野是局部的，相比全局自注意力可能限制模型捕获长程依赖的能力。为兼顾效率和全局感受野，FastViT 提出在不采用自注意力的阶段引入大卷积核的深度卷积来拓展感受野。这一想法计算上非常高效，可显著提升网络早期层的感受野和特征表达能力，而不会像自注意力那样带来高昂的计算和内存代价。具体而言，FastViT 在每个 FFN 模块中和每次降采样的 Patch Embedding 模块中加入了 深度卷积核（可视作ConvNeXt样式的卷积FFN改进）。 Table4 对比了使用大核卷积与插入自注意力层在早期阶段的影响：如 V4 vs V3，当将所有阶段都用RepMixer（无自注意力）但加大核卷积（V4）对比前三阶段含自注意力的模型（V3），V4模型Top-1仅低0.6%，但参数减少11%、推理延迟降低约2.3倍。类似地，V2 vs V4：V2（含部分自注意力）比V4（无注意力，大卷积）参数多20%、延迟高7%，准确率反而相近。这表明大核卷积能够在无需自注意力的情况下取得与注意力模型相当的精度，却能显著降低计算延时，是提升早期阶段性能的高效方案。在Table1 的消融实验最后两行也可以看到：在FFN中使用大卷积核可提升FastViT-S12准确率约0.5%，在Patch Embedding中再使用大卷积核再增益0.4%，两者合计**+0.9% Top-1**，移动端延迟仅增加约0.1-0.2ms。 FastViT 的 FFN 和补丁嵌入层的具体结构如 Figure2 所示，其模式与 ConvNeXt 块类似但有关键区别：（1）使用 Batch Normalization 替代 Layer Normalization，这是因为 BN 能与前一层融合，在推理时无额外开销，而LayerNorm在ConvNeXt实现中需要张量维度变换，不利于部署效率。（2）大卷积核增加了FFN块的卷积特性。卷积型FFN模块被认为比原始全连接FFN对扰动更加鲁棒。因此在FFN中引入大核卷积不仅扩大全局感受野，也提升了模型鲁棒性。总体而言，引入大卷积核是一种高效提升模型性能与稳健性的手段，在FastViT中发挥了重要作用。 4 Experiments4.1 Image Classification作者首先在 ImageNet-1K 图像分类数据集上验证了 FastViT 的性能。 ImageNet-1K 包含约128万张训练图像和5万张验证图像，涵盖1000个类别。 所有 FastViT 模型均按照标准训练配置进行训练：训练周期300个epoch，优化器使用AdamW，权重衰减0.05，批量大小1024，余弦学习率调度（预热5个epoch，峰值学习率 ）。 采用 PyTorch 的 timm 库实现训练并使用8张 NVIDIA A100 GPU。 对于分辨率提升至384×384的变体，作者在224模型基础上额外微调30个epoch（学习率，权重衰减，batch size 512）。 推理延迟测量方面：移动端在 iPhone 12 Pro Max 上使用 Core ML Tools 将模型转为CoreML格式执行，batch size=1，并取100次运行的中位数延迟；GPU端使用 TensorRT 将模型导出后在NVIDIA RTX-2080Ti上测试(batch size=8取中位数)。 与SOTA模型比较： Table5 汇总了 FastViT 与近期代表模型在ImageNet-1K上的性能和效率对比。为公平起见，作者对ConvNeXt的实现进行了优化（移除了一些不必要的reshape操作）以提升其部署效率，同时排除了一些无法成功导出到CoreML或TensorRT的模型（表中以“-”标注）。从表中可以看到，在桌面GPU和移动设备两种平台下，FastViT 相较同时代模型都取得了最佳的准确率-延迟折中。【例如，FastViT-MA36 模型（256×256输入）在 Top-1准确率 83.9% 时，参数量42.7M、FLOPs 7.9G，GPU延迟6.7ms、手机延迟4.5ms，分别比 ConvNeXt-B 提高了1.9×（手机）和 2.0×（GPU）的速度】。在相似准确率84.9%的情况下，FastViT-MA36 的GPU延迟与 NFNet-F1【1】相当，但参数量仅为其33.3%、FLOPs仅为其49.9%、移动端推理更是快42.8%。对于小模型，FastViT-S12（79.8% Top-1）相比MobileOne-S4【57】在iPhone上快26.3%，在2080Ti上快26.9%，但准确率略低0.4%。总体来看，无论是低延迟还是高准确率区间，FastViT 系列均全面覆盖并超越了之前的 CNN 和 Transformer 模型。 Table6展示了使用知识蒸馏（distillation）训练时，各模型在ImageNet上的性能对比。作者按照 DeiT提出的硬蒸馏方案进行训练：教师模型选用 RegNet-16GF（即16GFLOPs的RegNet），将教师的硬分类结果作为伪标签。所有 FastViT 模型蒸馏训练同样跑满300个epoch，并未像部分方法那样额外添加一个蒸馏用的分类头，而是直接利用蒸馏标签训练原有头。结果显示 FastViT 在蒸馏设置下进一步提升了性能，并超越了最新高效模型 EfficientFormer等。【例如 FastViT-SA24 蒸馏后Top-1达到83.4%，与EfficientFormer-L7相当，但FastViT-SA24参数量仅为后者的1/3.8（20.6M vs 82.1M），FLOPs仅为其约37%（3.8G vs 10.2G），推理延迟也低2.7×以上】。可见，在蒸馏增强下FastViT的小模型取得了接近大模型的准确率，同时保持了明显的效率优势。 4.2 Robustness Evaluation作者进一步评估了 FastViT 在分布外数据和图像扰动下的性能。具体采用了四个常用的鲁棒性基准： (i) ImageNet-A：收集了使ResNet易出错的真实图像； (ii) ImageNet-R：ImageNet类别的艺术化和卡通渲染图像集； (iii) ImageNet-Sketch：ImageNet类别对应的手绘黑白线稿集； (iv) ImageNet-C：对ImageNet验证集施加高斯噪声、模糊等一系列合成扰动得到的腐败集（有15种失真×5个强度）。 评估指标方面，对ImageNet-C使用平均腐败误差 (mean corruption error, mCE，值越低越好)，对其余三个数据集报告Top-1准确率 (值越高越好)。所有模型均使用开源实现在这些基准上测试，并确保只使用ImageNet-1K预训练（无其他数据）以公平比较。 Table7给出了不同模型在以上鲁棒性测试的结果（按FLOPs分组）。可以看到，FastViT 系列在鲁棒性-效率上表现出色：相比纯CNN模型，其对腐败和分布外数据更稳健，而与强调Transformer鲁棒性的近期模型（如 ConvNeXt、RVT等）相比，FastViT 明显更快且精度相当甚至更优。例如，在相近FLOPs组中，FastViT-SA36 的ImageNet-C误差为51.8（越低越好），显著优于ResNet-50 (65.5)和ConvNeXt-T (53.2)；FastViT-SA36 在ImageNet-A上准确率32.3%，仅略低于参数更大的ConvNeXt-S (31.2%) 和EffNet-B4 (26.3%)，在ImageNet-R和Sketch上则达到与ConvNeXt-S相当的48.1%和35.8%。FastViT-MA36（83.9% ImageNet精度）在各鲁棒性指标上甚至全面媲美ConvNeXt-S（83.1%精度，参数多出6M，FLOPs多10%），表现出更高的腐败鲁棒性和相近的分布外鲁棒性。作者将FastViT的高鲁棒性归功于架构设计上的选择：例如§3.2.3提到的大卷积核卷积与自注意力的结合，能够提升模型对纹理变化和噪声的适应性。此外，FastViT 使用的卷积型FFN模块本身也较标准FFN更稳健。综上，FastViT 在不牺牲速度的前提下，实现了优于同级模型的鲁棒性，在实际应用中能更好地应对不可预测的输入扰动。 4.3 3D Hand Mesh EstimationFastViT 架构在 3D手部网格回归 任务中同样表现优异。当前实时3D手部重建方法往往在CNN主干后附加复杂的网格回归模块（如基于Graph或Transformer的姿态回归层）。常用主干包括ResNet或HRNet系列，这些CNN在大部分硬件上都有良好优化，但沉重的回归头使整体延迟仍然较高。作者提出利用 FastViT 作为高效主干，同时尽量简化回归头：在FastViT提取的特征上直接用一个轻量级MLP回归 MANO 手部参数（不引入图形模型等额外计算）。在训练策略上，只使用ImageNet-1K预训练权重并仅在 FreiHAND 数据集上微调，不借助额外的合成数据，以突出模型自身的效果。 Table8列出了 FreiHAND 基准测试上的指标对比。在“实时”方法中（例如MobileHand、MobRecon等都是追求实时速度的轻量模型），FastViT 提供的方案达到最优的精度-速度权衡：它在关键指标（如顶点位置误差、命中率等）上略优于之前的最佳模型，同时在推理速度上远超对手——在相同硬件上，FastViT 模型比MobileHand快1.9倍，比最新的MobRecon快2.8倍。这充分说明了FastViT在三维重建任务中的潜力：通过减少主干延迟和简化回归头，可实现在几乎不损失精度的情况下大幅提升运行效率，非常适合嵌入式或移动端的实时3D感知应用。 4.4 Semantic Segmentation and Object Detection语义分割： 作者在 ADE20K 数据集上评估了FastViT作为分割模型主干的效果。ADE20K包含2万张训练图像和2千张验证图像，涵盖150个语义类别。实验采用 Semantic FPN 作为分割解码头，并使用与PoolFormer论文相同的训练配置（如80k步数、学习率等）。所有模型的主干都初始化为各自ImageNet-1K预训练权重。由于分割需处理高分辨率输入，这里统一在 图像裁块上评估各模型主干的 FLOPs 和延迟（GPU上使用batch=2来模拟高分辨率推理并获取稳定延迟）。 Table9 列出了若干主干在ADE20K上的分割结果和性能指标。可以看到，FastViT 系列相比同尺寸的CNN或Transformer主干取得了更高的 mIoU 和更低的推理延迟。例如，FastViT-MA36 主干在分割任务中达到 44.6% mIoU，较同级的 PoolFormer-M36 提高了5.2%，并且移动端延迟降低约1.5倍（24.8ms降至16.3ms）；在GPU上FastViT-MA36的主干延迟仅8.2ms，显著小于ResNet-101（4.6ms）或PoolFormer-M36（41.4ms）等。这说明FastViT不仅在分类，就算作为分割模型的特征提取主干也能提供更优的精度和更快的推理，有效提升分割模型的整体速度。 目标检测： 作者还在 MS COCO 检测数据集上验证了FastViT用于检测的性能。COCO包含118k训练图像和5k验证图像（80类目标）。实验采用经典的 Mask R-CNN框架，并使用1x调度（12个训练epoch）。各模型主干均用各自ImageNet预训练权重初始化。在评估时，同样将各模型在 输入下的主干延迟在GPU(batch=2)和移动端进行测量。 结果如Table10：FastViT 系列在不同延迟/精度等级下均达到SOTA级别。特别地，FastViT-MA36 主干在检测中取得了 45.1 的APbbox（边界框平均精度）和 40.5 的APmask（实例分割平均精度），与同等设定下当前最佳的CMT-S主干（APb=44.6, APm=40.7）性能非常接近，但FastViT-MA36在推理速度上对CMT-S形成碾压：GPU端快2.4倍，移动端快4.3倍（CMT-S主干GPU延时≈19.9ms vs FastViT≈8.2ms；移动端≈70.9ms vs 16.3ms）。同时我们看到，FastViT中等尺寸模型（如SA24、SA36）在Mask R-CNN上的检测精度也全面超过了同级别的ResNet-50/101和PoolFormer等主干。综上，在高水平下，FastViT主干可以以显著更低的延迟实现与最先进CNN/Transformer相当甚至更优的检测和分割性能，使高精度视觉模型更贴近实际部署要求。 5. Conclusion作者提出了一种通用的混合视觉Transformer FastViT，能在移动设备和桌面GPU等多种平台上都实现高效推理。通过结构重参数化、训练时过参数化和大卷积核等策略，FastViT 在保持模型容量和准确率的同时，将推理时不必要的开销降至最低，达到了目前视觉模型中顶尖的精度-效率组合。实验结果证明，FastViT 不仅在标准ImageNet分类上超越已有模型，在检测、分割、3D重建等多任务上也展现出强大的性能与泛化性，且具备优异的鲁棒性。未来，作者的工作为高效视觉Transformer的设计提供了新思路，有望激发更多在模型重参数化和混合架构方面的研究和应用。 个人思考这篇论文通过一系列巧妙的结构改进，在不牺牲性能的前提下大幅提升了视觉Transformer的推理效率，让我们看到架构设计对实际部署的重要性。FastViT 的 RepMixer 模块令人印象深刻——它抓住了跳跃连接带来的内存访问瓶颈，通过重参数化彻底将其移除，实现了Transformer架构在移动设备上的极致加速。这一思路不局限于视觉Transformer，未来或许也能迁移到其它模型（如NLP中的Transformer、语音模型等）中，通过移除或重构冗余结构来优化推理延迟。 另一个启发点在于训练与推理分离的设计。FastViT 的一些结构在训练时“加冕”，在推理时“隐去”（例如训练时过参数化分支、BatchNorm层等），充分利用了训练阶段的冗余来提升模型容量，又不增加推理成本。这种“所见非所得”的设计理念可以在其他模型压缩领域探索，例如结合量化和蒸馏进一步压缩模型，同时保持性能。 FastViT 展示的大卷积核卷积在Transformer框架下的应用也值得关注。这表明即使在Transformer大行其道的今天，卷积算子仍有不可替代的价值：大核卷积提供的鲁棒性和局部建模能力与自注意力形成互补。未来模型或许可以自适应地决定在哪些层采用卷积、哪些层采用注意力，以充分利用二者优势。 潜在的应用方面，FastViT 的超高速度和良好精度使其非常适合部署在对时延敏感的场景，如移动端实时AR、视频实时处理、无人机或自动驾驶中的视觉模块等。在这些场景中，每毫秒的节省都十分宝贵，FastViT 提供了一个用小模型实现大作为的范例。 当然，FastViT 也有值得改进之处。例如，在极致追求速度时，FastViT 小模型的准确率仍落后于更大模型，如何在不大幅增加推理延迟的情况下进一步提升精度是一个挑战。此外，FastViT 的重参数化策略使模型训练相对复杂一些（比如需要精心调试融合分支的训练），未来或许可以探索自动化的结构重参数化方法，降低设计和调参成本。 总的来说，FastViT 体现了通过结构创新优化模型的巨大潜力。它让我们意识到，不仅要关注算法层面的改进，模型结构设计和训练-推理策略的融合也能带来革命性的提升。相信随着这一方向的深入研究，会有更多既高速又高精度的模型涌现，推动视觉AI在资源受限设备上的应用发展。","link":"/2025/11/01/Notes/FastViT_Note/"},{"title":"MAR: MEDICAL ASYMMETRIC RETRIEVER FOR EFFICIENT CHINESE MEDICAL DENSE RETRIEVAL 阅读笔记","text":"金培晟 Jarfield 0 摘要本文提出中文医学文本嵌入基准 MedTEB，覆盖三类贴近真实场景的任务：检索、重排序与医学同义句相似度（STS）。在构建过程中，我们采用基于多模型的 LLM 标注流程以提升数据质量。对强通用嵌入模型在 MedTEB 上的评测显示，该基准具有面向领域且更具挑战性的检索评测价值。基于此，我们提出医学非对称检索器（MAR）：将查询与文档编码解耦，在线用轻量查询编码器实现低延迟，离线用更强大的（LLM-based）文档编码器保证检索质量。为优化这一非对称架构，我们引入两阶段训练框架：（1）查询编码器对齐；（2） 联合微调。实验表明，MAR 在 MedTEB 上取得SOTA 性能，同时其推理速度与小型 BERT 类嵌入模型相当，兼顾准确率与效率，适用于真实的中文医学检索场景。代码、数据与模型将公开以促进后续研究。 1 引言 背景与动机 向量表征（embedding）是现代 NLP 的基础，广泛用于检索、重排、分类，并是 RAG 的关键组件。 医疗等专门领域中，LLM 往往缺乏深度专家知识；因此需要准确且低延迟地访问医学知识，以提升临床决策支持并减少 RAG 幻觉：领域化、低延迟的医学嵌入是刚需。 现状与差距 通用嵌入模型进展很快，但中文医学文本嵌入关注不足。 现有基准如 C-MTEB 仅含两套中文医学检索数据，且存在标注噪声与假负例。 当前强力嵌入多为LLM-based：性能强但延迟与算力成本高，限制实时医疗问答等敏感场景。 基准贡献（MedTEB） MedTEB：包含检索、重排序、医学同义句 STS三类全新整理任务，并纳入两套公开数据。 使用LLM 驱动的标注流程提升标签质量。 评测显示：即便强大的通用嵌入模型在 MedTEB 上表现也不佳，证明其难度与领域针对性。 方法贡献（MAR 非对称检索器 + 两阶段训练） MAR：轻量查询编码器在线服务以降延迟，更强文档编码器离线建库保性能。 两阶段优化：（1）查询编码器对齐；（2）联合微调，直接面向检索目标。 结果：如Figure1所示： 2 相关工作 Embedding Models 发展过程：从无监督对比预训练（如 Contriever）到大规模指令化/对比预训练（E5、GTE、BGE），通用语义表示显著提升。 Decoder-only 兴起：Qwen3-Embedding、bge-en-icl、NV-Embed 等在 MTEB 上达 SOTA，证明仅解码器架构也能产出强嵌入。 瓶颈：多数 LLM-based 模型参数量大、延迟高、开销重，不适合实时医疗检索等延迟敏感场景：需要轻量且有效的领域嵌入。 Medical Embedding Benchmarks MTEB 提供跨语种多任务评测；C-MTEB 纳入多项中文数据集。 已有医学相关数据（CmedqaRetrieval、MedicalRetrieval、CMedQA-v1/v2 等）虽被收录，但检索任务存在标注噪声与假负例，相对仅重排序更可靠。 缺少系统化、可信度高的中文医学嵌入基准 ：MedTEB 即为弥补此缺口。 Asymmetric Architecture 两大路线： (1) 裁剪+蒸馏：如 KALE，从大编码器裁剪层得到轻量查询塔，用 L2/KL 等对齐损失蒸馏教师知识； (2) 异构编码器：如 ScalingNote、HotelMatch，查询/文档用不同架构或模态，通过对齐学习提升检索效果。 本文不同做法： (1) 文档塔用 decoder-only，天然适配异构对齐； (2) 提出两阶段对齐框架（查询对齐+联合微调），直接面向检索优化； (3) 不做高维投影：保持原始低维、检索更高效。 3 MEDTEB 中文医学嵌入基准稀缺；现有 CmedqaRetrieval、MedicalRetrieval 多源自问答配对，忽略跨样本的潜在相关答案，医学领域“主题强度”又放大了假阴性风险。 在问答配对式数据中，假阴性就是：本应相关的答案被标成负例，模型检索对了却被评测判错。医学场景尤甚——同一疾病/药物常有大量可通用的专业回答，配对之外的有效答案在标注里被遗漏而默认为负。例：查询“莫西沙星能和布洛芬一起吃吗？”标注只认 A 医生为正，把 B/C 医生在其他贴里给出的同样正确用药原则都记作负；模型检到 B/C 仍被判错。 实验依据：LLM 预标注显示——MedicalRetrieval 每条查询平均 8.6 个“被标负但可能相关”的候选；CmedqaRetrieval 约 19 个。 MedTEB：三项新任务（Retrieval / Reranking / Synonym STS）+ 两个人工核验公开数据（CMedQA-v1/-v2-reranking）。 3.1 CONSTRUCTION METHOD Retrieval： 多检索器召回 + 多 LLM 共识标注；数据为 （真实匿名查询）、（标注后语料）、。 与 AIR-Bench 做法的区别：(i) 医疗域、(ii) 真实查询、(iii) 多 LLM + 大候选池缓解误标负与未标注正。 Rerank： 同样用多 LLM 标注，得正集 、负集 ；构造三元组 。 STS： 先建医学同义词词表；对每个 ，GPT-4o 生成：（同义保义）、（同义改义）、（非同义改义）。 采样配对成 ，检验细粒度同义理解。 3.2 EVALUATION OF EXISTING EMBEDDING MODELS 数据规模（Table 1）： 关键发现（Table 2）： 通用嵌入在 CMedQA 与新任务：85.15 vs 57.85：新任务更具挑战、医学域欠发达。 Spearman ρ=0.354, p=0.215（≫0.05）：新任务非冗余，能从新视角评估模型。 decoder-only 模型，如 Qwen3-Embedding-8B 在新任务平均 64.52；但延迟与算力成本高，限制真实应用。 4 MEDICAL ASYMMETRIC RETRIEVER鉴于现有模型在 MedTEB 上的局限性，作者提出非对称嵌入架构 + 两阶段训练：离线用更强的文档编码器（Doc Encoder）对全量语料向量化并建库；在线仅用轻量查询编码器（Query Encoder）编码用户查询进行近似向量检索。 4.1 HIGH-QUALITY DATA CONSTRUCTION 难点：医学“主题强度”导致潜在正样本多，传统 hard negative 工作流程常失效： Top-k 挖负易夹带假阴性（未标注但相关的文档）； 阈值过滤决策边界模糊； 对庞大候选池做全量 LLM 标注成本过高。 多样性感知三步管线： 清洗与匿名化：从公开资源汇集中文医学语料 ，并从线上服务收集真实匿名查询 ；去隐私、正则清洗、格式规范化。 去重与多样化：基于动态向量索引（先建索引、再以相似度阈值过滤）去语义近重复，并按主题做均衡采样，降低冗余、提升长尾覆盖。 候选内精标：对每个 由多检索器融合召回 Top-50 候选，再用 GPT-4o（多 LLM 共识）标注可靠正/负，最终产出约 50 万 检索三元组 。 自对齐数据（服务于非对称对齐）： 查询侧： 共 2.8M； 文档侧： 共 5.6M； 其中正样由“文本与自身”构造，负样采用批内负为主。 4.2 INDEPENDENT INITIALIZATION为给两塔注入领域知识并提供稳健起点，先训练一个对称双塔（Query = Doc 结构一致）： 查询编码器（三级预训练 → 监督微调） RetroMAE 预训练：编码器/轻量解码器异步掩码；编码器产出句向量，解码器做 MLM 重构。语料：6000 万条中文医疗问答无监督语料。 无监督 InfoNCE 预训练：损失为 InfoNCE（温度 τ 可学习）。将标题–正文视作正对 ，同 batch 其余样本作批内负。 有监督 InfoNCE 微调：在 §4 构建的高质量数据 + MedTEB 训练划分（Retrieval、Rerank、CMedQA v1/v2、Synonym STS）上，用 InfoNCE 端到端优化检索表征。 文档编码器（大模型微调 + 多维嵌入） LoRA 微调：以 Qwen3-4B / Qwen3-8B 为底座，rank=32，α=64，在控制算力的同时保持性能。 MRL（Matryoshka Representation Learning）：训练嵌套维度集合 ，在每个目标维度上各自计算 InfoNCE，并对所有维度的损失求平均。推理时可截断到前 m 维以匹配轻量查询塔的向量维度，兼顾精度/延迟/索引大小的部署弹性。 初始化后得到一个 性能稳健的对称表示空间，为后续的 非对称对齐（Stage I） 与 联合微调（Stage II） 打下基础。 4.3 ASYMMETRIC EMBEDDING ARCHITECTURE 范式： 离线：Doc Encoder 对全集语料向量化，构建向量索引（可用余弦/内积）； 在线：Query Encoder 编码用户查询，进行 ANN 检索（HNSW/FAISS 等实现可替换）。 难点：轻量查询塔与强力文档塔嵌入空间天然不对齐。 4.3.1 ASYMMETRIC STAGE I: QUERY ENCODER ALIGNMENT 做法：冻结 Doc Encoder（教师），仅更新 Query Encoder（学生）。训练数据采用上节的自对齐集合与检索候选池中的正/负。 目标函数（混合对齐）： Asym-InfoNCE（相对排序对齐） 其中 , ， 为余弦相似（向量已归一化）， 为温度， 为负样数量（批内负为主）。在自对齐样本上， 与 为同一文本，可稳定拉近两塔同源表示。 MSE（绝对位置对齐） 直接惩罚同一文本在两塔中的向量距离，补充“坐标级”的对齐约束。 联合损失 直觉：Asym-InfoNCE 提供相对排序信号，MSE 提供绝对位置信号；两者协同可更稳地缩小两塔语义鸿沟，避免仅有排序信号时的漂移。 4.4 ASYMMETRIC STAGE II: JOINT FINE-TUNING 目标：在初步对齐的基础上，端到端提升检索判别力。 做法：解冻两塔，仅以 Asym-InfoNCE 为训练目标；结合批内负 + 硬负丰富难例。 结果：最终得到 MAR（Medical Asymmetric Retriever）系列模型——在 MedTEB 上取得强精度，同时在线仅跑轻量查询塔、离线预计算文档塔，实现SOTA 级准确率 × 小模型级 QPS/延迟的现实可用折中。 5 实验5.1 SETUP Models Query：Medical-Embedder-base（由 gte-multilingual-mlm-base 初始化，≈0.3B 参数）。 Document：Medical-Embedder-4B / -8B（在 Qwen3-4B / Qwen3-8B 上微调）。 Asymmetric 变体：MAR-0.3B-4B（0.3B Query + 4B Doc）、MAR-0.3B-8B（0.3B Query + 8B Doc）。 Baselines：BGE、GTE、Qwen3-Embedding、Conan-embedding-v1、stella-base-zh-v3-1792d 等。 Training Data（统一）第4节的高质量微调集 + MedTEB 训练划分（Retrieval / Rerank / CMedQA / Synonym STS）。即便部分baseline在预训练阶段见过 CMedQA，仍显式纳入以避免该任务潜在性能下降。 Implementation 检索评估：用 FAISS 近邻检索。 总exposure量对齐：所有对称基线 fine-tune 2 个 epoch，与本文提出的MAR总exposure量匹配。 计算资源：32× A100-40GB。 5.2 MAIN RESULTS ON MEDTEB SOTA： MAR-0.3B-4B 平均 78.13，MAR-0.3B-8B 平均 78.94；均超过最强基线 gte-Qwen2-1.5B-instruct = 77.61（decoder-only），且Query 仅 0.3B。 扩展性： 将文档塔 4B → 8B，平均提升 +0.81；查询时延不变（Query 仍 0.3B）。 5.3 ASYMMETRIC vs. SYMMETRIC 非对称方案逼近大文档模型的上限（8B 对称 65.63 vs. 8B 非对称 65.21)，同时远超轻量对称；放大 Doc 塔能单向提升精度，且不增加查询时延。 5.4 ABLATION STUDY5.4.1 Training Design 独立初始化缺一不可：去掉 Query init → 59.66；去掉 Doc init → 50.26；完整模型 64.38。 先训对称双塔为非对称阶段提供更强起点至关重要。 两阶段都重要：无 Query 对齐 → 51.07；无联合微调 → 55.49；完整 64.38。 对齐阶段让学生 Query 学到教师 Doc 的空间，联合微调让两塔适配下游检索。 损失设计（Stage I）：去 MSE → 63.57；去对比项 → 64.03；两者并用 64.38（最佳）。 **相对排序（Asym-InfoNCE）+ 绝对位置（MSE）**均有贡献。 5.4.2 Query Alignment Data 仅做 Stage-I：用“fine-tuning 数据”57.26 &gt; “alignment 数据”55.49。 再接 Stage-II：先用 alignment 数据的最终表现 64.38 &gt; 先用 fine-tuning 数据 60.49。 对齐专用数据更能铺好表示空间、抬高性能上限；直接用下游数据做对齐易过早收敛、表达不佳。 5.4.3 Alternatives to Efficient Retrieval KALE：55.05；Wang &amp; Lyu (2023)：53.13；ScalingNote：49.49（均为非对称）。 Distill-from-4B（对称蒸馏学生）：62.72。 MAR-0.3B-4B：64.38（最佳）。 编码器裁剪/分数蒸馏在decoder-only场景下适配有限；直接把强 Doc 作为教师并保留其离线向量，可避免蒸馏信息损失，检索效果更强。 6 结论本文发布中文医学嵌入新基准 MedTEB，并提出面向低延迟医疗检索的非对称模型 MAR（轻量查询侧 + 大规模文档侧，二阶段对齐/微调）。在 MedTEB 上取得 SOTA，并开源基准、模型与训练流程，为真实医疗 RAG 落地与领域嵌入研究提供实践方案与起点。 我的思考读完这篇论文，我学习到了一条可落地的路线：把“强能力”放在离线的文档塔，把“低时延”放在在线的查询塔。这种非对称设计天然适配需要实时响应的检索/RAG；迁移到多模态时，也只需把文档塔换成强 VLM/MLLM，查询侧保留轻量编码头即可。 同时，本文的训练策略也很关键：先做一次对称初始化给两塔注入领域知识；随后 Stage-I 冻结文档塔，只训查询塔，用 **Asym-InfoNCE（相对排序）+ MSE（绝对位置）**把两塔空间拉齐；最后 Stage-II 解冻两塔，仅用 Asym-InfoNCE 面向检索目标端到端优化。这里的一个小技巧值得照搬——对齐专用数据：/ 先把“同一文本”在两塔里对齐到同一位置，再去学正负间隔；而文档塔用 LoRA 降算力、配合 MRL 训练多维嵌套表示，后续就能在不改查询塔的前提下，只放大文档塔来换取精度，查询时延几乎不变。 数据层面，我明白高质量数据的价值，通过在 Top-K 候选里精做数据——多检索器召回→多 LLM 一致性复核→必要时允许多正样 的做法，专门对付医学场景里“主题强度高导致假阴性多”的难点。做多模态时，同样可以考虑把一致性扩展成 VLM+LLM 双通道。 最后是如何从实验结果中分析：一方面，针对精度&amp;效率双线（nDCG@10/Recall@k + QPS/显存/延迟），探究“文档塔变大、查询时延不变”的收益；另一方面，通过轻量诊断——假阴性（每查询的可疑负例数+抽检通过率）、错误切片（实体/同义/否定等维度），以及对齐前后余弦相似度确认空间收敛。","link":"/2025/10/26/Notes/MAR_Note/"},{"title":"大语言模型（阅读笔记1）","text":"第一章 引言1.1 语言模型的发展历程语言模型的发展大致分为四个阶段：统计语言模型解决了最初的词序预测问题，但受限于稀疏性与维度灾难；神经语言模型引入词嵌入，改进语义表征；预训练模型（如 BERT、GPT-1）借助大规模无监督学习与微调，提升了上下文理解；最终演进到大语言模型（如 GPT-3/4），通过规模扩展展现出“涌现能力”。这里最关键的转折点是 Transformer 的提出，它既解决了长程依赖问题，又适配了并行计算，为后续 LLM 奠定了基础。 1.2 大语言模型的能力特点","link":"/2025/10/08/Notes/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%951/"},{"title":"数据结构习题详解ustc","text":"金培晟 Jarfield 第六章 树和二叉树 6.1【题目】试分别绘出具有 3 个结点的树和 3 个结点的二叉树的所有不同形态。 【解析】 “树”默认有根的普通树（孩子个数不限、孩子之间无左右次序），忽略结点标号，仅按形态区分。3 个结点时共有 2 种不同形态：一条链、以及根有两个孩子的“星形”。 “二叉树”中左右次序有别（有序二叉树），因此 3 个结点时的不同形态数为 Catalan 数 ：一棵满二叉三结点树 + 四种单支/折线形（分别位于左或右，或呈“折”形）。 【答案】 一、具有3个结点的树 123456●|●|● 1234 ● / \\● ● 二、具有 3 个结点的二叉树 12345 ● / \\● ● 123456 ● / ● /● 1234567 ● /● \\ ● 1234567● \\ ● \\ ● 1234567● \\ ● /● 6.4【题目】一个深度为 H 的满 k 叉树有如下性质：第 H 层上所有结点都是叶子结点，其余各层上每个结点都有 k 棵非空子树。如果从 1 开始按自上而下、自左向右的次序对全部结点编号，问： (1) 各层的结点数目是多少？ (2) 编号为 i 的结点的父结点(若存在)的编号是多少？ (3) 编号为 i 的结点的第 j 个孩子(若存在)的编号是多少？ (4) 编号为 i 的结点有右兄弟的条件是什么？其右兄弟的编号是多少？ 【解析】 假定：层次从 1 开始计数（根在第 1 层，叶在第 H 层），k≥2。 满 k 叉树第 t 层结点数为 ；到第 t 层的累计结点数为 。 全树结点总数 。 按层序（自上而下、左到右）从 1 编号时，可把“父与其 k 个孩子”看作连续块： 结点 i 的第 j 个孩子（1≤j≤k）紧跟在前面 个孩子之后，因此位于下标 。 除根外，每个结点 i（i≥2）都处在其父亲的 k 个孩子块中；块的起点决定了父编号公式。 右兄弟：同一父亲的孩子相邻，结点 i 不是其父亲的第 k 个孩子时，右兄弟就是 i+1。用模运算表示即 （且 i&gt;1）。 【答案】 (1) 第 t 层（t=1,2,…,H）的结点数：。 累计到第 t 层：；全树总数：。 (2) 结点 i 的父结点编号：不存在(3) 结点 i 的第 j 个孩子（1≤j≤k）编号：存在条件：结点 i 不在第 H 层（等价于 ）。 (4) 右兄弟存在的充要条件： 且 （即 i 不是其父亲的第 k 个孩子）。 此时右兄弟编号：。 6.5【题目】 已知一棵度为 的树中有 个度为 1 的结点， 个度为 2 的结点，…， 个度为 的结点，问该树中有多少个叶子结点？ 【解析】 约定：“度”为孩子数，叶子结点的度为 0，记叶子数 。 设全树结点总数 。树的边数：。 另一方面，各内部结点的出度和即边数：。 两式相等并整理：即叶子结点数与各度结点数的关系式。 【答案】（其中 为叶子结点个数） 6.11【题目】 已知某二叉树的中序序列为 DCBGEAHFIJK，后序序列为 DCEGBFHKJIA，请画出该二叉树。 【解析】 根结点 = 后序最后一项 A。 按 A 切分中序：左中序 DCBGE，右中序 HFIJK。去掉 A 的后序为 DCEGBFHKJI，对应分为左后序 DCEGB 与右后序 FHKJI。 左子树（中序 DCBGE，后序 DCEGB）： 根 B（左后序末）。 以 B 切分中序：左 DC、右 GE；后序相应为 DC、EG。 左部分根 C（DC 末），其左孩子 D；右部分根 G（EG 末），其右孩子 E。 右子树（中序 HFIJK，后序 FHKJI）： 根 I（右后序末）。 以 I 切分中序：左 HF、右 JK；后序相应为 FH、KJ。 左部分根 H（FH 末），其右孩子 F；右部分根 J（KJ 末），其右孩子 K。 【答案】 1234567 A / \\ B I / \\ / \\ C G H J / \\ \\ \\D E F K 6.14【题目】 假设某个电文由 8 个字母组成，每个字母在电文中出现的次数分别为 。试回答： (1) 画出 Huffman 树； (2) 写出每个字母的 Huffman 编码； (3) 最优二进制编码后，电文的二进制位数。 【解析】 采用 Huffman 贪心合并（每次取当前最小的两棵子树合并）。约定：左分支记 ，右分支记 。 合并过程（频次作为权重）： （根） 说明：Huffman 在相同权的分支选择上可能有多种形态，编码不唯一，但总位数最优值相同。 【答案】 (1) Huffman 树（左 / 右 ）： 1234567891011 (100) / \\ (40) (60) / \\ / \\b:19 g:21 (28) e:32 / \\ (11) (17) / \\ / \\ (5) d:6 a:7 h:10 / \\ c:2 f:3 (2) Huffman 编码（左 / 右 ）： 字母 频次 编码 码长 频次×码长 a 7 1010 b 19 00 c 2 10000 d 6 1001 e 32 11 f 3 10001 g 21 01 h 10 1011 (3) 电文最优总二进制位数：总位数","link":"/2025/11/01/TA/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%A0%E9%A2%98%E8%AF%A6%E8%A7%A3ustc/"},{"title":"大语言模型（阅读笔记2）","text":"金培晟 Jarfield 第二章 基础介绍大语言模型是指在海量无标注文本数据上进行预训练得到的大型预训练语言模型,例如 GPT-3 ,PaLM 和 LLaMA 。目前大语言模型所需要具有的最小参数规模还没有一个明确的参考标准,但是大语言模型通常是指参数规模达到百亿、千亿甚至万亿的模型;也有部分工作认为经过大规模数据预训练(显著 多于传统预训练模型如 BERT 所需要的训练数据)的数十亿参数级别的模型也可 以称之为大语言模型(如 LLaMA-2 7B)。对于大语言模型,本书泛指具有超大规模参数或者经过超大规模数据训练所得到的语言模型。与传统语言模型相比,大语言模型的构建过程涉及到更为复杂的训练方法,进而展现出了强大的自然语言 理解能力和复杂任务求解能力(通过文本生成的形式)。为了帮助读者了解大语言模型的工作原理,本部分将介绍大语言模型的构建过程、扩展法则(Scaling Law)、涌现能力(Emergent Abilities),然后将介绍 GPT 系列模型的研发历程。 2.1 大语言模型的构建过程从机器学习视角看，大语言模型可以被理解为一种基于 Transformer 的大规模参数化函数，训练的本质是用数据去拟合这个函数的参数，使其在更广泛的任务上“泛化”而非只对某一类任务过拟合。为了逼近“通用任务求解器”的目标，业界通常把研发流程划分为两条紧密衔接的主线：首先进行大规模预训练，让模型获得覆盖面尽可能广的“常识性世界知识”；随后进行指令微调与人类对齐，让这种知识以问答和对话的外显形式变得更可用、更符合人类偏好。 2.1.1 大规模预训练预训练可以看作是为参数寻找一个具有普适性的“初值点”。实践中，“解码器架构 + 预测下一个词”的语言建模目标已经被反复验证为稳健有效：它并不要求明确的下游标签，而是依靠大规模无标注文本，让模型在不断预测下一个词的过程中，压缩并内化关于语法、语义、事实与常识的统计规律。这里“压缩世界知识”的比喻并非夸张——如果把真实数据分布看作一台信息源，那么好的预训练就是在尽可能大的样本覆盖下，用有限参数去逼近这台信息源的可预测部分；当参数有限、数据嘈杂或清洗不足时，压缩就会发生损失，模型表现为“可约误差”偏高。 因此，数据质量与清洗程度并非琐碎的工程细节，而是决定预训练上限的核心约束：来源多样但尽量同质化的切分、严格的去重与有害内容过滤、稳定的词元化策略以及对批处理/数据加载的工程优化，都会直接作用于训练的可控性与收敛速度。很多“训练不稳定”的现象，与其说是优化器超参数选择不当，不如说是数据分布在训练过程中呈现出隐蔽的漂移与重复，从而诱发损失曲线的异常形态。换言之，预训练的难点往往不在“算法是否高级”，而在“数据是否干净、覆盖是否到位、流程是否可复现”。 2.1.2 指令微调与人类对齐预训练模型擅长“补全”，但未必擅长“遵循任务指令”；它知道很多，却不一定会以人类偏好的方式表达与决策。**指令微调（SFT）**的作用在于把这种知识外化为“问—答”范式，使模型学会在较少示例下理解任务意图与输出格式。本质上，SFT是一种模仿学习：给定标准示范，模型在预训练的表示之上学会“如何回答”。这一步通常不需要太多数据，但对数据质量和覆盖面非常敏感——覆盖的任务形态越多、写法越规范，微调后模型的泛化表现越稳定。 在人机交互场景里，仅有“会做题”还不够，“价值观对齐”同样重要。**基于人类反馈的强化学习（RLHF）**通过偏好数据训练一个奖励模型，再用策略优化方法把“更可取的生成”推向更高概率。直觉上，这是把“答案好坏”的判别器装进了优化闭环中，使得模型不仅会答，还尽量按“人类认为合适的方式”去答。需要注意的是，对齐并不是一次性的动作：它依赖偏好数据的稳定性与一致性，也依赖后续评测对有害指令、幻觉与鲁棒性的持续约束。换句话说，SFT 与对齐并非“尾声”，而更像是“接口”，把预训练的能力包装成可以被社会安全地消费的能力。 2.2 扩展法则（Scaling Laws）大语言模型的一个关键洞见在于：在相当宽的规模区间内，性能提升更多来自“规模”的系统性扩展，而非局部结构的小修小改。为此，人们尝试用可量化的关系式去刻画三类核心变量——参数规模 、数据规模 与计算量 ——如何共同决定交叉熵损失 。这些关系式并非纯理论推导，而是基于大规模实验拟合得到的“经验定律”；但正因为它们在不同设置下表现出相当稳健的趋势，才足以成为资源配置与路线选择的北极星。 2.2.1 KM 扩展法则（Kaplan 等）在给定算力预算下，模型损失与 呈现幂律依赖：其中 ，损失单位是以 为底的 nat。更具启发性的是，把损失分解为可以把 解释为由数据分布本身决定、几乎不可再压缩的“熵项”，而后项对应的是可通过扩展与优化降低的“可约误差”。当我们把 分别替换为 时，就得到三条“沿不同方向扩展”的收益曲线：在同一数量级的增幅下，它们下降的速度不同，暗示了哪个“瓶颈”更值得优先投入。直观地说，如果你继续加大参数规模而验证损失已经接近停滞，那么真正的约束大概率已经从“模型容量”转移到了“数据量与数据洁净度”。 值得注意的是，KM 法则默认了“单因素变动而其他因素不构成瓶颈”的实验设定。这是它能被拟合为幂律的前提，也是实际工程中容易被破坏的地方：比如当上下文长度、优化器、并行策略或数据去重策略发生显著改变时，幂指数与标度常数都可能漂移。因此，使用 KM 法则更好的方式不是“把一组指数套到一切规模段”，而是对自己数据/框架的有效区间进行再拟合与再验证。 2.2.2 Chinchilla 扩展法则（Hoffmann 等）在更大的规模范围内，“参数—数据—算力”的三元关系可以被写成与 KM 不同，Chinchilla 更强调在给定算力 下，参数与数据应如何协同扩展。若近似取训练计算量 ，则可推得算力固定时的“最优配比”随算力按幂次增长：其中 由 决定。这个结果有两个直接含义：其一，算力既不是只拿来“堆大模型”，也不是只拿来“喂更多数据”，而应按 的弹性去配比；其二，历史上那种显著偏向增大 而忽视 的做法，会把你带到一个“参数很大但事实上处于欠训练（undertrained）”的区域，从而白白消耗算力而不能等额换取损失下降。换到更直观的语言，就是不要把算力花在“把大模型训练在相对小的数据上”，而应把一部分算力换成“更长的数据流”，让模型的表达能力得到真正的激活。 把 KM 与 Chinchilla 放在一起看，它们并不矛盾：KM 让我们看到“规模扩展”的总体趋势与可约误差的可降解性；Chinchilla 则提醒我们，算力是联立约束，应在 与 间取得可计算的均衡。实际工作中，人们常在自家数据与框架上重新拟合一条“本地的扩展法则”，用少量候选配比做验证，再把最优配比推广到更大规模——这比直接照搬文献中的常数与指数更稳妥。 2.2.3 关于扩展法则的讨论在给出 KM 与 Chinchilla 两条经验定律之后，更关键的问题是：我们究竟能在多大程度上“预测”更大规模下的表现，以及这种可预测性能否从“语言建模损失”过渡到“真实任务指标”。这两点分别对应可预测的扩展与任务层面的可预测性。 可预测的扩展（Predictable Scaling）指的是：在若干前提保持一致时，用“小成本”的观测去推断“大成本”的结果。最常见的两种做法，一是用小模型在固定数据/优化设定下拟合出 与规模（或算力）的幂律，再将这条曲线外推到更大的 、、；二是用大模型的早期训练轨迹去估计其收敛附近的损失水平。若以 Chinchilla 的形式为例，固定数据分布与训练策略不变时，意味着在对数坐标上存在近似线性的“斜率”可供外推；又例如把训练计算量看作主变量，许多设定下都可用刻画早—中—晚期的平滑下降，这就使得早期点的偏差成为识别异常运行的有效手段：当某次训练在相同设定下明显“跑在”既有幂律带之外（无论过高或过低），往往提示数据加载、去重、学习率或并行策略出了问题。与之相对的，是“收益递减”的不可避免性——指数项 或 的衰减让后期加算力带来的损失下降越来越慢，曲线逼近 的同时，单位算力的边际收益下降。但这并不意味着“扩得不值”：已有结果表明，即便总损失接近不可约熵项 （见 2.2 中的分解），表征质量与下游迁移仍会随规模得到实质改善 [21]。真正的难题反而来自数据本身的有限性：在现实世界可获取的高质量文本逐步枯竭时，扩展法则需要在重复采样与合成数据的情形下重新刻画——重复会抬高有效熵、增加过拟合风险，而合成又可能引入分布偏移；如何在保持幂律可预测性的同时控制这两类偏差，是未来工程化扩展必须面对的约束。 转向任务层面的可预测性，问题更微妙。KM 与 Chinchilla 主要是在语言建模损失（如平均交叉熵/困惑度）上拟合幂律，这个量对“整体能力”是一个平滑且可加的度量，因此在宏观上“损失更低→下游更好”常常成立。然而，损失与任务指标之间的映射并非线性且处处单调：当任务需要额外的格式对齐、长上下文推断、工具使用或特定的推理策略时，单纯降低语言建模损失可能无法同步转化为任务增益；极端时还会出现所谓“逆向扩展”（inverse scaling）现象，即困惑度下降而特定任务指标反而变差 [37]。直观解释是：语言建模损失捕捉的是“整体分布拟合”的进步，但具体任务的度量函数可能对某些稀有模式、提示格式或推理链高度敏感，这些“局部能力”的阈值行为会破坏平滑的幂律对应关系。也因此，文献中有观察到：某些能力（如编码能力）可以被扩展曲线较好地预测，而另一些（如上下文学习能力）则呈现规模阈值与突现特征，只有当模型越过某个表征/上下文容量门槛时才会“突然出现”，在门槛之下很难从损失的微小改变量中提前预告 [35, 23]。 综合而言，扩展法则最可靠的用法不是把它当作一条放之四海而皆准的“通用曲线”，而是把它当作本地化、可复现的经验定律：在固定的数据分布、清洗与优化策略下，用小模型或早期阶段拟合出自己的 与常数，再在此基础上外推与告警；在任务层面，则应为关键评测建立损失—指标的桥接函数（哪怕是经验性的），并特别关注那些历史上已知存在“阈值/涌现/逆向扩展”的类别，把它们从“可平滑外推”的集合里分离出来。如此一来，扩展法则既能在资源规划上指引“把算力花在刀刃上”，也不会在能力评测上给予过度承诺。至于数据稀缺时代的外推，重复采样与高质量合成的混合策略可能是务实之选，但它要求我们在每一次设定变化后重新校准幂律参数，确保“可预测的扩展”名副其实。 2.3 涌现能力“涌现能力”常被非形式化地描述为：某些在小模型上几乎不可见或仅略高于随机水平的能力，在模型扩展到一定规模后出现突增，其任务表现呈现出接近“相变”的跃迁。尽管关于是否“真正涌现”仍有争议，但这个术语抓住了一个现象：当参数规模、数据规模与训练配方共同跨过某些隐含门槛时，模型的可用性在特定任务上会发生质变而非仅是量变。 2.3.1 代表性的涌现能力上下文学习（In-context Learning, ICL） 是在不更新参数的前提下，仅靠提示中的指令与少量示例，让模型按“示范—迁移”的方式产出正确答案。GPT-3 首次系统展示了这一点：当参数足够大时，模型能把输入视作“一次性训练集”，在同一前向中完成“读题—归纳—作答”的隐式过程。然而 ICL 的阈值并非对所有任务一致：较小模型在低复杂度任务（如有限位数加减法）上能显出雏形，但在需要丰富世界知识或稀疏语料的任务（例如波斯语问答任务）上，甚至不能表现良好。一种可能的解释是，ICL依赖于表征容量与上下文记忆/检索的协同：只有当注意力与中间表示可以在上下文中“拟合出一个小模型”时，**隐式的“元学习”**才会显著发生。 指令遵循（Instruction Following） 强调“按自然语言意图行事”。它通常通过指令微调（SFT）习得：给定多任务、标准化的“指令—响应”对，模型学会解析意图、约束风格并按格式输出。与 ICL 相比，指令遵循在规模门槛上更可控：高质量、多样化的指令数据会显著降低“按照指令回答”的最小规模要求，但最终上限仍受模型容量与任务难度制约。经验上，当参数跨过中等规模（如数十亿到数百亿）并配合良好的指令数据，零样本或少样本的复杂推理评测（如 BBH）才会出现稳定提升；而在小模型（如 2B 量级）上，尽管能够通过使用高质量指令数据微调的方式习得一定的通用指令遵循能力，但更多局限于格式驱动和浅层任务（摘要、抽取等）。 逐步推理（Step-by-step Reasoning） 依赖“把思考过程写出来”的提示策略（CoT）。当模型具备足够的表征与上下文承载能力时，为了解决涉及多个推理步骤的复杂任务，通过在提示中显式引入中间步骤，会把“隐式推理”外化成“可读推理”，进而显著提升正确求解的概率。该能力同样呈现规模阈值：在较小参数量下，CoT 往往“写慢不写明”；当模型扩至更大规模时，CoT 的增益在复杂数学问题的推理基准上尤为明显。直观地讲，CoT 让模型把“长链依赖”分解成一系列短依赖，这要求足够长的上下文、稳定的中间表示、以及对错误步的抑制，而这些都与规模、训练数据的多样性和清洁度正相关。 需要强调的是，所谓“临界规模”尚未被统一界定。高质量、覆盖面广且对齐良好的预训练与指令数据，可以把门槛整体下移，使得较小模型也能在一定程度上表现出上述三种能力；反之，数据分布稀疏、清洗不足或训练不稳，则会抬高或模糊这些门槛。换而言之，随着数据规模的扩展和数据质量的提升，过去对于参数规模的要求在一点点放宽。另一方面，许多公开报告只覆盖了稀疏的几个模型规模点（如 8B、62B、540B），这会让能力曲线在可视化上显得更“跳”，而缺少模型规模处于中间点的平滑过渡。 2.3.2 涌现能力与扩展法则的关系从度量上看，扩展法则与涌现能力关注的是同一头“大象”的不同侧面。扩展法则多以语言建模损失 (L) 为代理，得到的是平滑、可外推的下降曲线，并自然呈现边际收益递减；涌现能力多以具体任务指标为准绳，所见往往是台阶式的跃升。二者之所以看似矛盾，核心在于：平滑的“分布拟合进步”如何投射到“离散的任务成功”。当任务度量本身是不连续的（如“通过/未通过”）、或当我们只观察到稀疏的模型规模点时，微小而持续的损失改进就可能在某一刻跨过任务阈值，表现为“从几乎不可用到可用”的跳变；在更密集的规模取样和更连续的指标设计下，这种跳变会变得温和。 这并不削弱“涌现”的实践意义。真实用户以离散的满意度感知系统：能否全对、能否运行、能否遵循格式，往往比“平均损失再降一点”更重要。因此，在资源规划层面，扩展法则提供“把算力花在刀刃上”的可预测基线；在产品与评测层面，涌现提醒我们关注那些阈值敏感的能力族（ICL、指令遵循、链式推理），并通过数据与提示工程降低它们的门槛。值得一提的相关现象是“顿悟”（grokking）：在固定规模与数据下，训练中后期性能从“记忆训练集”突然转向“强泛化”。它提示我们，能力的跃迁既可能随规模发生，也可能随训练进程发生，两者共同塑造了“看似突然”的提升。 将两种视角合并起来，一个务实的做法是：先在固定数据与优化设定下，用小规模或早期阶段拟合本地的扩展曲线，作为监控与外推的锚点；再针对关键任务建立损失—指标的桥接（哪怕是经验性的阈值图谱），并对 ICL、CoT、指令遵循等“敏感能力”进行专门的提示与数据设计。当曲线与阈值同时被管理，我们既能享受规模带来的可预期收益，也能更早、更稳地触发那些“看得见的质变”。 2.4 GPT 系列模型的技术演变自 2022 年 11 月 ChatGPT 上线以来，围绕“大语言模型”的社会与学术关注迅速聚拢。若把 GPT 系列视作这一波技术更迭的主轴，它的演进几乎可以用两件事概括：一是用仅解码器（Decoder-Only）的 Transformer 做“下一个词”预测，把广泛的世界知识压进参数化的生成模型；二是扩展语言模型的规模以及扩展预训练数据的规模，并配套更好的训练与对齐工艺。前者给出统一的学习范式，后者把能力从“可用”推向“好用”。 2.4.1 早期探索：GPT-1 与 GPT-2OpenAI 最初也尝试过循环网络，但随着 Transformer 的出现，路线迅速转向“基于文本的生成式预训练”。GPT-1（2018）确立了“解码器 + 语言建模目标”的基本骨架；当时的模型规模接近 BERT-Base，能力上尚不足以成为通用求解器，因此仍依赖“预训练 + 有监督微调”的范式。与同时期的 BERT 不同，GPT 放弃了编码器侧的“掩码理解”，选择只用解码器，直接优化生成式目标。 GPT-2（2019）在架构不变的前提下把参数扩到 1.5B，并用大规模网页语料（WebText）做无监督预训练，核心主张是：尽量用统一的生成式目标去覆盖尽可能多的任务。形式上，可以把多任务学习写成 ——若把“任务”也写成自然语言，所有任务都可还原为“给定条件文本继续生成”的同一问题。于是，只要语言模型能高保真地“复原”世界文本的统计结构，它在许多下游上就能够自然显露能力。这一思想为后续的“提示学习（Prompting）”与“在同一目标下统一训练与使用”奠定了语义基础。 2.4.2 规模扩展：GPT-3 的里程碑虽然 GPT-2 曾被寄予“无监督多任务学习器”的厚望，但在众多任务上仍逊于显式微调。GPT-3（2020）把参数直接推到175B，并在论文中系统提出 上下文学习（ICL） ：**不给梯度、仅靠提示中的“指令 + 少量示例”，模型即可在一次前向中完成“读题—归纳—作答”的隐式适配。**与第 2.2 节的扩展法则相呼应，GPT-3 以规模跃升换来了更稳定的少样本泛化，**从“预训练语言模型”跨进了“通用大模型”的门槛。**训练与使用也因此可以在统一的语言建模框架下叙述：训练是在上下文条件下预测后续词元，使用则是把“任务与示例”包装进上下文，令模型在同一目标上完成推理。 2.4.3 能力增强：代码数据与人类对齐在 GPT-3 之后，能力提升主要沿两条路径展开。其一是代码数据：**Codex（2021）**在海量 GitHub 数据上微调，显著改善了编程与数学类推理。直觉上，代码数据的**强结构化、长距离依赖与可执行性**为模型提供了更清晰的“对齐信号”，也让“推理链”更易被内化；随后“文本-代码对比学习”的嵌入方法进一步拓展了检索与匹配类能力。公开信息显示，GPT-3.5是在“代码强化”的基础上继续演化而来，这提示我们：预训练的数据版图并不局限于自然语言文本，凡是能稳定刻画因果与逻辑结构的语料都可能提升泛化。 其二是人类对齐：OpenAI 早期的“从人类偏好中学习”与 PPO 策略优化汇入 InstructGPT（2022） 的 RLHF 流水线：先用成对偏好训练奖励模型，再以策略优化把“更可取的输出”推到更高概率。这样，模型不仅“会做题”，还更按照人类意图与安全边界去做题。需要注意的是，OpenAI 文档里更常用“监督微调（SFT）”来称呼 RLHF 的第一步；而“指令微调”更多是社区术语。总体而言，SFT 打开“按指令办事”的门，RLHF 把“怎么更合意地办事”纳入优化闭环。 2.4.4 性能跃升：ChatGPT、GPT-4 与多模态**ChatGPT（2022.11）**把上述积累装进对话式产品形态：在预训练与指令微调的基础上，以对话数据统一格式，针对多轮上下文与安全对齐做专项优化。它把“泛化能力、对话记忆与安全边界”揉进了一个可交互的接口，因此一经发布便成为能力与可用性的拐点。 GPT-4（2023）*进一步把输入扩展到*图文多模态，在一系列人类考试与综合评测上显著领先，并将“可预测扩展”的理念落到训练流程：用更少计算的早期点外推最终性能，据此做训练监控与停训决策；同时叠加“红队”与安全奖励等干预，降低有害输出与幻觉风险。随后发布的 GPT-4V 聚焦视觉能力的安全部署**，而 GPT-4 Turbo 在更长上下文（至 128K）、更低推理成本、函数调用与可重复输出等方面扩展了工程上限，并与 Assistants API 一同把“模型—工具—应用”的生态闭环打通。由此，大模型不只是一种能力，更是一套可被嵌入应用的能力操作系统。当然，局限从未消失：幻觉、对抗脆弱性、隐私与合规风险仍在；因此 OpenAI 采取迭代式部署，以阶段性对齐与评测来换取在能力提升与安全之间的动态平衡。换句话说，GPT 系列的演进并非“单调做大”，而是在统一目标下的系统工程：用规模换潜力，用数据与对齐把潜力固化成可控的产品能力，再用生态与工具把能力外延成可持续的应用形态。","link":"/2025/10/14/Notes/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%952/"}],"tags":[{"name":"高效模型","slug":"高效模型","link":"/tags/%E9%AB%98%E6%95%88%E6%A8%A1%E5%9E%8B/"},{"name":"视觉Transformer","slug":"视觉Transformer","link":"/tags/%E8%A7%86%E8%A7%89Transformer/"},{"name":"网络压缩","slug":"网络压缩","link":"/tags/%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9/"},{"name":"知识蒸馏","slug":"知识蒸馏","link":"/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/"},{"name":"大语言模型","slug":"大语言模型","link":"/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"categories":[{"name":"论文解读","slug":"论文解读","link":"/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"习题解答","slug":"习题解答","link":"/categories/%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94/"}],"pages":[{"title":"about","text":"柯南一枚","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}